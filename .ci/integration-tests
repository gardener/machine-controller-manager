#!/usr/bin/env bash

# Copyright (c) 2018 SAP SE or an SAP affiliate company. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

mkdir -p dev
mkdir -p logs

# TODO add support for azure and openstack
declare -a cloud_providers=("aws" "gcp")
declare failed_logs=""
num_of_existing_nodes=1

if [ -n "$1" ]; then
    printf "Local test"
    LOCAL_TEST=true

    if [[ (-z "$2") || (-z "$3") ]]; then
        printf ".ci/integration-tests LOCAL_TEST {PATH_TO_LOCAL_CLI} {PATH_TO_SECRETS_CACHE_FILE}"
        exit 1
    fi

    cli_path=$2
    cache_path=$3
    Objects_path=.ci/sample-objects
else
    cli_path=/cc/utils/cli.py
    Objects_path=$SOURCE_PATH/.ci/sample-objects
fi

for cp in "${cloud_providers[@]}"; do
    declare control_kubeconfig_${cp}=dev/control_kubeconfig_${cp}.yaml
    declare target_kubeconfig_${cp}=dev/target_kubeconfig_${cp}.yaml

    if [ -n "$LOCAL_TEST" ]; then
        ${cli_path} config --cache-file $cache_path attribute --cfg-type kubernetes --cfg-name mcm-ci-${cp}-control --key kubeconfig > dev/control_kubeconfig_${cp}.yaml
        ${cli_path} config --cache-file $cache_path attribute --cfg-type kubernetes --cfg-name mcm-ci-${cp}-target --key kubeconfig > dev/target_kubeconfig_${cp}.yaml
    else
        ${cli_path} config attribute --cfg-type kubernetes --cfg-name mcm-ci-${cp}-control --key kubeconfig > dev/control_kubeconfig_${cp}.yaml
        ${cli_path} config attribute --cfg-type kubernetes --cfg-name mcm-ci-${cp}-target --key kubeconfig > dev/target_kubeconfig_${cp}.yaml
    fi
done


############################################## <Initialization> ##############################################

function check_cluster_state() {
    printf "\t\t\t----- Checking Test Environment -------\n"

    printf "\nChecking existance of machine objects\n"
    # Wait 60mins for any existing PRs to cleanup machines
    hf_wait_on "hf_num_of_objects" mach 0 3600
    printf "No machine objects in control test cluster\n"

    printf "\nChecking existance of node objects\n"
    # Wait 60mins for any existing PRs to cleanup nodes
    hf_wait_on "hf_num_of_ready_nodes" nodes 0 3600
    printf "No additional node objects in target test cluster\n"

    printf "\nCluster state looks clean\n"
}

function setup_environment() {
    printf "\n\t\t\t----- Setup Test Environment --------\n"

    # If not a local test then install kubectl
    if [ -z "$LOCAL_TEST" ]; then
        printf "\nDownloading and installing kubectl\n"
        curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.16.0/bin/linux/amd64/kubectl
        chmod +x ./kubectl
        mv ./kubectl /usr/local/bin/kubectl
        printf "Successfully installed kubectl\n"
    fi

    # If not a local test then install jq
    if [ -z "$LOCAL_TEST" ]; then
        printf "\nDownloading and installing jq\n"
        apt-get -y update
        apt-get -y install jq
        printf "Successfully installed jq\n"
    fi

    # If not a local test then install aws-cli
    if [ -z "$LOCAL_TEST" ]; then
        printf "\nDownloading and setting up aws-cli\n"
        pip3 install awscli --upgrade

        export AWS_SECRET_ACCESS_KEY="$(kubectl -n aws --kubeconfig=dev/control_kubeconfig_aws.yaml get secrets secret-v1 -o json --context=control | jq -r '.data.providerSecretAccessKey' | base64 -d)"
        export AWS_ACCESS_KEY_ID="$(kubectl -n aws --kubeconfig=dev/control_kubeconfig_aws.yaml get secrets secret-v1 -o json --context=control | jq -r '.data.providerAccessKeyId' | base64 -d)"
        export AWS_DEFAULT_REGION="eu-west-1"

        printf "Successfully setup aws-cli\n"
    fi

    # If not a local test then install gcloud-cli
    if [ -z "$LOCAL_TEST" ]; then
        printf "\nDownloading and setting up gcloud-cli\n"
        curl https://sdk.cloud.google.com > install.sh
        bash install.sh --disable-prompts &> /dev/null
        source /root/google-cloud-sdk/path.bash.inc

        kubectl -n gcp --kubeconfig=dev/control_kubeconfig_gcp.yaml get secrets secret-v1 -o json --context=control | jq -r '.data.serviceAccountJSON' | base64 -d > servicekey.json

        ProjectID="$(kubectl -n gcp --kubeconfig=dev/control_kubeconfig_gcp.yaml get secrets secret-v1 -o json --context=control | jq -r '.data.serviceAccountJSON' | base64 -d | jq -r '.project_id')"
        
        gcloud auth activate-service-account --key-file servicekey.json
        gcloud config set project "$ProjectID"
        gcloud config set compute/zone europe-west1-b
        gcloud config set disable_usage_reporting True

        printf "Successfully setup gcloud-cli\n"
    fi
    
    printf "\nBuilding MCM binary\n"
    if GO111MODULE=on go build -mod=vendor -i cmd/machine-controller-manager/controller_manager.go; then
        printf "Go build Successful\n"
    else
        printf "Go build Failure\n"
        terminate_script
    fi
}

# run_controller runs MCM controller in background
# it expects the port to be passed as a parameter
function run_controller() {
    printf "\nRunning MCM in background\n"
    ./controller_manager \
        --control-kubeconfig=dev/control_kubeconfig_${provider}.yaml \
        --target-kubeconfig=dev/target_kubeconfig_${provider}.yaml \
        --namespace=${provider} \
        --port=$1 \
        --safety-up=2 \
        --safety-down=1 \
        --machine-creation-timeout=20m \
        --machine-drain-timeout=5m \
        --machine-health-timeout=10m \
        --machine-safety-apiserver-statuscheck-timeout=30s \
        --machine-safety-apiserver-statuscheck-period=1m \
        --machine-safety-orphan-vms-period=30m \
        --machine-safety-overshooting-period=300ms \
        --leader-elect-lease-duration=1m \
        --leader-elect-renew-deadline=30s \
        --leader-elect-retry-period=15s \
        --v=2 > logs/${provider}-mcm.out 2>&1 &
}

############################################## </Intialization> ##############################################
############################################## <HelperFunction> ##############################################

function hf_object_create() {
    output=$(kubectl apply -f "$Objects_path"/"$1" --context=control 2>&1)
    outputSucccess=$(echo "$output" | grep created -c)
    if [[ outputSucccess -ne 1 ]]; then
        printf "\tFailed: To create object. Exiting Test to avoid further conflicts.\n"
        printf "\tCreate error message: %s" "$output"
        terminate_script
    fi

    printf "\n\t%s" "$output"
}

function hf_object_configure() {
    output=$(kubectl apply -f "$Objects_path"/"$1" --context=control 2>&1)
    outputSucccess=$(echo "$output" | grep configured -c)
    if [[ outputSucccess -ne 1 ]]; then
        printf "\tFailed: To apply object. Exiting Test to avoid further conflicts.\n"
        printf "\tApply error message: %s" "$output"
        terminate_script
    fi

    printf "\n\t%s" "$output"
}

function hf_object_delete() {
    output=$(kubectl delete -f "$Objects_path"/"$1" --context=control 2>&1)
    outputSucccess=$(echo "$output" | grep deleted -c)
    if [[ outputSucccess -ne 1 ]]; then
        printf "\tFailed: To delete object. Exiting Test to avoid further conflicts.\n"
        printf "\tDelete error message: %s" "$output"
        terminate_script
    fi

    printf "\n\t%s" "$output"
}

function hf_scale_deploy() {
    output=$(kubectl scale deployment "$1" --replicas="$2" --context=control --namespace="$provider" 2>&1)
    outputSucccess=$(echo "$output" | grep scaled -c)
    if [[ outputSucccess -ne 1 ]]; then
        printf "\tFailed: To scale deployment object. Exiting Test to avoid further conflicts.\n"
        printf "\tScale error message: %s" "$output"
        printf "Clean-up script for $provider has failed. Kindly manually cleanup the cluster"
    fi
}

function hf_num_of_objects() {
    output=$(kubectl -n $provider get "$1" --context=control 2>&1)
    if [[ $output == *"No resources found."* ]]; then
        return 0
    fi
    object_count=$(echo "$output" | wc -l)
    ((object_count--))

    return "$object_count"
}

function hf_num_of_ready_nodes() {
    output=$(kubectl get "$1" --context=target 2>&1)
    ready_count=$(echo "$output" | tr " " "\n" | grep ^Ready -c)

    return $((ready_count-num_of_existing_nodes))
}

function hf_wait_on() {
    function_name=$1
    function_param=$2
    count_to_match=$3
    seconds_to_wait=$4
    iteration_count=$(($seconds_to_wait/30))

    while
        "$function_name" "$function_param"
        ret=$?
        [[ $ret -ne $count_to_match ]]
    do
        sleep 30
        ((iteration_count--))

        # Exit script when timeout occurs
        if [ $iteration_count -le 0 ]; then
            printf "\tFailed: Timeout occured while waiting for operation. Exiting Test to avoid further conflicts.\n"
            printf "\tExecuting function: %s, %s" $function_name $function_param
            if [ -z "$5" ]; then
                terminate_script
            fi
        fi

    done
}

function hf_check_ms_freeze() {
    freeze_count=$(cat logs/${provider}-mcm.out | grep ' Froze MachineSet' | wc -l)
    if [[ freeze_count -eq 0 ]]; then
        printf "\tFailed: Freezing of machineSet failed. Exiting Test to avoid further conflicts.\n"
        terminate_script
    fi

    unfreeze_count=$(cat logs/${provider}-mcm.out | grep ' Unfroze MachineSet' | wc -l)
    if [[ unfreeze_count -eq 0 ]]; then
        printf "\tFailed: Unfreezing of machineSet failed. Exiting Test to avoid further conflicts.\n"
        terminate_script
    fi
}

function hf_orphan_resources() {

    if [ $1 == "aws" ]; then
        # Check if there are any VMs matching the tag exists.
        orphan_vms="$(aws ec2 describe-instances --filters "Name=tag-key,Values=mcm-ci-test/shoot-mcm-ci-aw-target" --query 'Reservations[*].Instances[?State.Name!=`terminated`].[InstanceId]' --output text)"
        num_orphan_vms="$( echo -n "$orphan_vms" | wc -l )"

        # Check if there are any disks matching the tag exists.
        orphan_disks="$(aws ec2 describe-volumes --filters "Name=tag-key,Values=mcm-ci-test/shoot-mcm-ci-aw-target" --query "Volumes[*].{ID:VolumeId}" --output text)"
        num_orphan_disks="$( echo -n "$orphan_disks" | wc -l )"

        if [[ "$num_orphan_vms" -gt 0 || "$num_orphan_disks" -gt 0 ]]; then
            printf "\n Failed: Found orphan resources. Orphan VMs: $num_orphan_vms. Orphan Disks: $num_orphan_disks" 
            
            if [ $num_orphan_vms -gt 0 ]; then
                printf "\n Deleting Orphan VMs: \n $orphan_vms"
                # Delete the orphan VMs.
                aws ec2 terminate-instances --instance-ids $orphan_vms
            fi

            if [ $num_orphan_disks -gt 0 ]; then
                printf "\n Deleting Orphan Disks: \n $orphan_disks"
                # Delete the orphan Disks.
                for line in $orphan_disks; do
                    aws ec2 delete-volume --volume-id $line
                done
            fi

            return 1
        fi
    fi

    if [ $1 == "gcp" ]; then
        # Check if there are any VMs matching the tag exists.
        orphan_vms="$(gcloud compute instances list --filter="tags.items=mcm-ci-test-shoot-mcm-ci-gc-target" --format='value(name)')"
        num_orphan_vms="$( echo "$orphan_vms" | wc -w )"

        # Check if there are any disks matching the tag exists.
        orphan_disks="$(gcloud compute disks list --filter="labels.mcm-ci-test:shoot-mcm-ci-gc-target" --format='value(name)')"
        num_orphan_disks="$( echo "$orphan_disks" | wc -w )"

        if [[ "$num_orphan_vms" -gt 0 || "$num_orphan_disks" -gt 0 ]]; then
            printf "\n Failed: Found orphan resources in GCP. Orphan VMs: $num_orphan_vms. Orphan Disks: $num_orphan_disks" 
            
            if [ $num_orphan_vms -gt 0 ]; then
                printf "\n Deleting Orphan VMs: \n $orphan_vms"
                # Delete the orphan VMs.
                for line in $orphan_vms; do                    
                    gcloud compute instances delete $line --quiet
                done
            fi

            if [ $num_orphan_disks -gt 0 ]; then
                printf "\n Deleting Orphan Disks: \n $orphan_disks"
                # Delete the orphan Disks.
                for line in $orphan_disks; do
                    gcloud compute disks delete $line --quiet
                done
            fi
            return 1
        fi
    fi

    printf "\n No orphan resources found.  \n"
    return 0
}

function terminate_script() {
    printf "\n\t\t\t----- Start of MCM Logs for $provider -----------\n"
    cat logs/${provider}-mcm.out
    printf "\n\t\t\t----- End of MCM logs for $provider -----------\n\n"
    printf "CI tests have failed for $provider. Re-check your PR."
    exit 1
}

############################################## </HelperFunction> #############################################
############################################## <TestCases> ###################################################
function tc_machine() {

    printf "\nStarting TestCase: %s" "${FUNCNAME[0]}"

    # Wait until 1 nodes have joined the cluster
    hf_object_create $provider/machine.yaml
    printf "\n\tWaiting 1800s for machine to join the cluster"
    hf_wait_on "hf_num_of_ready_nodes" nodes 1 1800

    # Wait for deletion of machine
    hf_object_delete $provider/machine.yaml
    printf "\n\tWaiting 1800s for machine to be deleted"
    hf_wait_on "hf_num_of_objects" mach 0 1800

    printf "\nCompleted TestCase\n"
}

function tc_machine_deployment() {

    printf "\nStarting TestCase: %s" "${FUNCNAME[0]}"

    # Wait until 3 nodes have joined the cluster
    hf_object_create $provider/md.yaml
    printf "\n\tWaiting 1800s for 3 machines to join the cluster"
    hf_wait_on "hf_num_of_ready_nodes" nodes 3 1800

    # Scale up the number of nodes to 6 and rapidly scale back to 2
    hf_object_configure $provider/md-scale-up.yaml
    printf "\n\tScale up the number of nodes to 6 and rapidly scale back to 2 leading to a freezing and unfreezing"
    sleep 20

    # Scale down the number of nodes to 2
    hf_object_configure $provider/md-scale-down.yaml
    printf "\n\tWaiting 1800s for machines to scale-down to 2"
    hf_wait_on "hf_num_of_ready_nodes" nodes 2 1800

    # Update the machines to v2 and double the number of replicas
    hf_object_configure $provider/md-update.yaml
    printf "\n\tWaiting 2400s for machines to upgrade to larger machine types and scale-up replicas to 4"
    hf_wait_on "hf_num_of_ready_nodes" nodes 4 2400

    # Delete the machine-deployment
    hf_object_delete $provider/md.yaml
    printf "\n\tWaiting 1800s for machine to be deleted"
    hf_wait_on "hf_num_of_objects" machdeploy 0 1800

    # Scaling from 6 to 2 should lead to freezing and unfreezing of machine-set
    printf "\n\tCheck for freezing and unfreezing of machine-set due to rapid scaleUp and scaleDown"
    hf_check_ms_freeze
    printf "\n\tFreezing and unfreezing of machineSet was observed"

    printf "\nCompleted TestCase\n"
}

function tc_check_orphan_resources() {
    
    printf "\nStarting TestCase: %s" "${FUNCNAME[0]}"
     
    # Check if there are any leftover resources.
    if ! hf_orphan_resources "$provider"; then
        printf "\n\t Failed: There are orphan resources in the cloud. Exiting the test."
        terminate_script
    fi

    printf "\nCompleted TestCase\n"
}

############################################## </TestCases> ##################################################
############################################## <CleanUp> #####################################################

function clean_up() {
    provider=$1
    export KUBECONFIG=dev/control_kubeconfig_${provider}.yaml:dev/target_kubeconfig_${provider}.yaml

    printf "\n\t\t\t----- CleanUp Test Environment -----\n"
    hf_scale_deploy machine-controller-manager 1

    if
        hf_num_of_objects machdeploy 0
        ret=$?
        [[ $ret -ne 0 ]]
    then
        # Delete the machine-deployment
        output=$(kubectl delete machdeploy --context=control --namespace="$provider" --all 2>&1)
        printf "\n\t%s" "$output"
        printf "\n\tWaiting 1800s for machine-deployment to be deleted"
        hf_wait_on "hf_num_of_objects" machdeploy 0 1800 $5
    fi

    if
        hf_num_of_objects machset 0
        ret=$?
        [[ $ret -ne 0 ]]
    then
        # Delete the machine-set
        output=$(kubectl delete machset --context=control --namespace="$provider" --all 2>&1)
        printf "\n\t%s" "$output"
        printf "\n\tWaiting 1800s for machine-set to be deleted"
        hf_wait_on "hf_num_of_objects" machdeploy 0 1800 $5
    fi

    if
        hf_num_of_objects mach 0
        ret=$?
        [[ $ret -ne 0 ]]
    then
        # Delete the machine
        output=$(kubectl delete mach --context=control --namespace="$provider" --all 2>&1)
        printf "\n\t%s" "$output"
        printf "\n\tWaiting 1800s for machine to be deleted"
        hf_wait_on "hf_num_of_objects" mach 0 1800 $5
    fi

    hf_scale_deploy machine-controller-manager 0
    printf "\n\t\t\t----- End of CleanUp Test Environment -----\n"
}

function cleanup_all_providers() {

    i=0
    for cp in "${cloud_providers[@]}"; do
        clean_up ${cp} > logs/${cp}-cleanup.out & pids[${i}]=$!
        printf "\n\t\t\tStarted clean-up for provider: $cp"
        ((i++))
    done

    # wait for all pids
    for pid in ${pids[*]}; do
        printf "\nWaiting for clean-ups to complete"
        wait $pid
    done
}

############################################## </CleanUp> ####################################################
############################################## <Modules> ########################################################

function test_cloud_provider() {
    provider=$1
    export KUBECONFIG=dev/control_kubeconfig_${provider}.yaml:dev/target_kubeconfig_${provider}.yaml

    run_controller $2
    check_cluster_state

    printf "\n\t\t\t----- Start of TestCases -----------\n"

    tc_machine
    tc_machine_deployment
    sleep 60 # Wait a bit for cloud-providers to reflect the correct state of the resources.
    tc_check_orphan_resources

    printf "\n\t\t\t----- End of TestCases -------------\n"

    printf "\n\t\t\t----- Start of MCM Logs for $provider -----------\n"
    cat logs/${provider}-mcm.out
    printf "\n\t\t\t----- End of MCM logs for $provider -----------\n"
}

function test_all_providers() {

    i=0
    for cp in "${cloud_providers[@]}"; do
        port=`expr 10258 + $i`
        test_cloud_provider ${cp} $port > logs/${cp}-it.out & pids[${i}]=$!
        printf "\n\t\t\tStarted test for provider: $cp on port-no: $port"
        ((i++))
    done

    # wait for all pids
    for pid in ${pids[*]}; do
        printf "\nWaiting for integration tests to complete"
        wait $pid
    done
}

function print_logs() {
    for cp in "${cloud_providers[@]}"; do
        printf "\n\t\t\t----- Start of Integration Test Logs for $cp -----------\n"
        cat logs/${cp}-it.out
        printf "\n\t\t\t----- End of Integration Test logs for $cp -----------\n"

        output=$(cat logs/${cp}-it.out)
        if [[ $output == *"CI tests have failed for"* ]]; then
            failed_logs="${failed_logs}\n\t- ${cp}"

            printf "\n\t\t\t----- Start of Clean-Up Logs for $cp -----------\n"
            cat logs/${cp}-cleanup.out
            printf "\n\t\t\t----- End of Clean-Up logs for $cp -----------\n"
        fi
    done
}

function print_results() {
    if [[ $failed_logs != "" ]]; then
        printf "\n\t\t\t----- Test failure logs -----------\n"
        printf "Integration test for the following cloud providers have failed. Kindly re-check your PR."
        echo -e ${failed_logs}
        printf "\n\t\t\t----- End of Test failure logs -----------\n"
        exit 1
    else
        printf "CI tests passed successfully"
    fi
}

############################################## </Modules> ####################################################
############################################## <Main> ########################################################

printf "\n\t\t\t----- Start of Test Script -----------\n"

setup_environment
test_all_providers
pkill controller_manager
cleanup_all_providers
print_logs
print_results

printf "\n\t\t\t----- End of Test Script -----------\n"

############################################## </Main> #######################################################
